{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99f43448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.574031267728\n",
      "0.134698573897\n",
      "0.029963223377\n",
      "0.079181246048\n",
      "0.045757490561\n",
      "--------------\n",
      "0.005351714939\n",
      "0.003659885335\n",
      "0.001990612219\n",
      "0.023996540734\n",
      "0.039311641490\n",
      "--------------\n",
      "(03_adams_john_1797.txt, 0.044190057362)\n",
      "(20_lincoln_1865.txt, 0.136596561747)\n",
      "(07_madison_1813.txt, 0.082936482104)\n",
      "(15_polk_1845.txt, 0.070347633806)\n",
      "(29_mckinley_1901.txt, 0.096775365055)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# Reading the 30 .txt files\n",
    "corpus_root = './US_Inaugural_Addresses'\n",
    "docs = {}\n",
    "for filename in os.listdir(corpus_root):\n",
    "    if filename.endswith('.txt'):\n",
    "        file = open(os.path.join(corpus_root, filename), \"r\", encoding='windows-1252')\n",
    "        doc = file.read().lower()\n",
    "        docs[filename] = doc\n",
    "        \n",
    "# Tokenizing the content of each file\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "tokenized_docs = {}\n",
    "for filename, doc in docs.items():\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    tokenized_docs[filename] = tokens\n",
    "\n",
    "# Removing stopwords\n",
    "sw = stopwords.words('english')\n",
    "docs_without_sw = {}\n",
    "for filename, tokens in tokenized_docs.items():\n",
    "    no_sw_tokens = [token for token in tokens if token not in sw]\n",
    "    docs_without_sw[filename] = no_sw_tokens\n",
    "\n",
    "# Performing stemming on the obtained tokens from each document\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_docs = {}\n",
    "for filename, tokens in docs_without_sw.items():\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    stemmed_docs[filename] = stemmed_tokens\n",
    "\n",
    "# Function to calculate IDF\n",
    "def getidf(token):\n",
    "    stemmer = PorterStemmer()\n",
    "    num_docs = len(docs_without_sw)\n",
    "    stemmed_token = stemmer.stem(token)\n",
    "    df = sum(1 for doc_tokens in stemmed_docs.values() if stemmed_token in doc_tokens)\n",
    "    if df > 0:\n",
    "        idf = math.log10(num_docs / df)\n",
    "        return idf\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "# Function to calculate IDF used in getweight and query without stemming the token, since the tokens will be intially stemmed in the respective functions\n",
    "def getidf_nostemming(token):\n",
    "    num_docs = len(stemmed_docs)\n",
    "    df = sum(1 for doc_tokens in stemmed_docs.values() if token in doc_tokens)\n",
    "    if df > 0:\n",
    "        idf = math.log10(num_docs / df)\n",
    "        return idf\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# Function to calculate TF-IDF weight\n",
    "def getweight(filename, token):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_token = stemmer.stem(token)\n",
    "    num_docs = len(stemmed_docs)\n",
    "    tf = stemmed_docs[filename].count(stemmed_token)\n",
    "    if tf == 0:\n",
    "        return 0\n",
    "    df = sum(1 for doc in stemmed_docs.values() if stemmed_token in doc)\n",
    "    idf = getidf_nostemming(stemmed_token)\n",
    "    tf_idf = (1 + math.log10(tf)) * idf\n",
    "    magnitude_doc = 0\n",
    "    x = set(stemmed_docs[filename])\n",
    "    for tok in x:\n",
    "        tf_term = stemmed_docs[filename].count(tok)\n",
    "        idf_term = getidf_nostemming(tok)\n",
    "        tf_idf_term = (1 + math.log10(tf_term)) * idf_term\n",
    "        magnitude_doc += tf_idf_term ** 2\n",
    "    magnitude_doc = math.sqrt(magnitude_doc)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if magnitude_doc == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate normalized TF-IDF\n",
    "    tf_idf /= magnitude_doc\n",
    "    \n",
    "    return tf_idf\n",
    "\n",
    "# Function to process a query and find the most relevant document\n",
    "def query(qstring):\n",
    "    stemmer = PorterStemmer()\n",
    "    query_words = qstring.lower().split()\n",
    "    stemmed_query_words = [stemmer.stem(word) for word in query_words]\n",
    "\n",
    "    query_vector = defaultdict(int)\n",
    "    for word in stemmed_query_words:\n",
    "        query_vector[word] += 1\n",
    "\n",
    "    # Compute TF-IDF for the query using ltc.lnc weighting scheme\n",
    "    query_tfidf = {}\n",
    "    for token, tf in query_vector.items():\n",
    "        tf_idf = (1 + math.log10(tf))\n",
    "        query_tfidf[token] = tf_idf\n",
    "    \n",
    "    max_score = -1\n",
    "    max_score_document = \"No documents found\"\n",
    "\n",
    "    # Compute cosine similarity between query and documents\n",
    "    for filename, tokens in stemmed_docs.items():\n",
    "        doc_vector = defaultdict(int)\n",
    "        for token in tokens:\n",
    "            doc_vector[token] += 1\n",
    "\n",
    "        # Compute TF-IDF for document using ltc.lnc weighting scheme\n",
    "        doc_tfidf = {}\n",
    "        for token, tf in doc_vector.items():\n",
    "            df = sum(1 for doc_tokens in stemmed_docs.values() if token in doc_tokens)\n",
    "            idf = math.log10(len(stemmed_docs) / df) if df > 0 else -1\n",
    "            tf_idf = (1 + math.log10(tf)) * idf\n",
    "            doc_tfidf[token] = tf_idf\n",
    "\n",
    "        # Calculate cosine similarity between query and document\n",
    "        dot_product = sum(query_tfidf[token] * doc_tfidf[token] for token in query_tfidf if token in doc_tfidf)\n",
    "        query_mag = math.sqrt(sum(query_tfidf[token] ** 2 for token in query_tfidf))\n",
    "        doc_mag = math.sqrt(sum(doc_tfidf[token] ** 2 for token in doc_tfidf))\n",
    "        \n",
    "        if query_mag != 0 and doc_mag != 0:\n",
    "            similarity_score = dot_product / (query_mag * doc_mag)\n",
    "        \n",
    "        if similarity_score > max_score:\n",
    "            max_score = similarity_score\n",
    "            max_score_document = filename\n",
    "\n",
    "    # Return the document with its score\n",
    "    return (max_score_document, max_score)\n",
    "\n",
    "print(\"%.12f\" % getidf('children'))\n",
    "print(\"%.12f\" % getidf('foreign'))\n",
    "print(\"%.12f\" % getidf('people'))\n",
    "print(\"%.12f\" % getidf('honor'))\n",
    "print(\"%.12f\" % getidf('great'))\n",
    "print(\"--------------\")\n",
    "print(\"%.12f\" % getweight('19_lincoln_1861.txt','constitution'))\n",
    "print(\"%.12f\" % getweight('23_hayes_1877.txt','public'))\n",
    "print(\"%.12f\" % getweight('25_cleveland_1885.txt','citizen'))\n",
    "print(\"%.12f\" % getweight('09_monroe_1821.txt','revenue'))\n",
    "print(\"%.12f\" % getweight('05_jefferson_1805.txt','press'))\n",
    "print(\"--------------\")\n",
    "print(\"(%s, %.12f)\" % query(\"pleasing people\"))\n",
    "print(\"(%s, %.12f)\" % query(\"war offenses\"))\n",
    "print(\"(%s, %.12f)\" % query(\"british war\"))\n",
    "print(\"(%s, %.12f)\" % query(\"texas government\"))\n",
    "print(\"(%s, %.12f)\" % query(\"cuba government\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
